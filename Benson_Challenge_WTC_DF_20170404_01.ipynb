{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime as dt\n",
    "import datetime\n",
    "\n",
    "import MP_functions as dfutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing & Loading Data\n",
    "\n",
    "Make sure to unzip the pickles.zip into the root folder for the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from load_pickles import df as pickle\n",
    "df = dfutil.open_dataframe_pickle('MTA_DATA_SPRING_2014_to_2016_FULL.pickle')\n",
    "dtp = dfutil.open_dataframe_pickle('daily_throughput.pickle')\n",
    "htp = dfutil.open_dataframe_pickle('hourly_throughput.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_daily_counts(row, max_counter, cols):\n",
    "    \"\"\"\n",
    "    cols - a list of columns to compute the difference from with format [x, y] = x-y\n",
    "    \"\"\"\n",
    "    counter = row[cols[0]] - row[cols[1]]\n",
    "    if counter < 0:\n",
    "        # May be counter is reversed?\n",
    "        counter = -counter\n",
    "    if counter > max_counter:\n",
    "#         print(row[cols[0]], row[cols[1]])\n",
    "        counter = min(row[cols[0]], row[cols[1]])\n",
    "    if counter > max_counter:\n",
    "        # Check it again to make sure we are not giving a counter that's too big\n",
    "        return 0\n",
    "    return counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cleanData(frame):\n",
    "    # Strip whitespace from column names\n",
    "    frame.columns = [s.strip() for s in frame.columns.values]\n",
    "\n",
    "    # Create datetime column with datetime datatype\n",
    "    frame['Datetime'] = pd.to_datetime(frame.DATE + ' ' + frame.TIME,\n",
    "                                        format = '%m/%d/%Y %H:%M:%S')\n",
    "    frame['Day_of_week'] = frame['Datetime'].apply(lambda row: row.strftime(\"%A\"))\n",
    "    \n",
    "    # Chain methods together to further clean data:\n",
    "        # drop old date and time columns\n",
    "        # rename columns\n",
    "    \n",
    "    dict_col_rename = {'C/A' : 'C_A', 'UNIT' : 'Unit', 'STATION' : 'Station', 'LINENAME' : 'Linename',\n",
    "                      'DIVISION' : 'Division', 'DESC' : 'Desc', 'ENTRIES' : 'Entries', 'EXITS' : 'Exits',\n",
    "                      'DATE' : 'Date'}    \n",
    "\n",
    "    frame2 = frame.rename(columns = dict_col_rename)\n",
    "    \n",
    "    # Check uniqueness of rows/indexes by getting counts.\n",
    "    (frame2\n",
    "     .groupby(['C_A', 'Unit', 'SCP', 'Station', 'Datetime'])\n",
    "     .count() #.Entries.count()\n",
    "     .reset_index()\n",
    "     .sort_values('Entries', ascending = False)\n",
    "    )\n",
    "    \n",
    "    # Drop duplicates.\n",
    "    return frame2.drop_duplicates(subset=['C_A', 'Unit', 'SCP', 'Station', 'Datetime'])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calcDailyThroughput(frame):\n",
    "    \n",
    "    # group daily entries and daily exits\n",
    "    daily_entries = (frame\n",
    "                      .groupby(['C_A', 'Unit', 'SCP', 'Station', 'Date'])\n",
    "                      .Entries\n",
    "                      .first()\n",
    "                      .reset_index()\n",
    "                    )\n",
    "\n",
    "    daily_exits = (frame\n",
    "                    .groupby(['C_A', 'Unit', 'SCP', 'Station', 'Date'])\n",
    "                    .Exits\n",
    "                    .first()\n",
    "                    .reset_index()\n",
    "                   )\n",
    "    \n",
    "    # Calculate the differences by day\n",
    "    daily_entries[[\"Prev_date\", \"Prev_entries\"]] = (daily_entries\n",
    "                                                     .groupby([\"C_A\", \"Unit\", \"SCP\", \"Station\"])[\"Date\", \"Entries\"]\n",
    "                                                     .transform(lambda grp: grp.shift(1)))\n",
    "\n",
    "    daily_exits[[\"Prev_date\", \"Prev_exits\"]]   = (daily_exits\n",
    "                                                   .groupby([\"C_A\", \"Unit\", \"SCP\", \"Station\"])[\"Date\", \"Exits\"]\n",
    "                                                   .transform(lambda grp: grp.shift(1)))\n",
    "\n",
    "    # Drop all the null values generated above\n",
    "    daily_entries.dropna(subset=[\"Prev_date\"], axis=0, inplace=True)\n",
    "    daily_exits.dropna(subset=[\"Prev_date\"], axis=0, inplace=True)\n",
    "\n",
    "\n",
    "    daily_entries[\"Daily_Entries\"] = daily_entries.apply(get_daily_counts, axis=1, args=(1000000, ['Entries', 'Prev_entries']))\n",
    "    daily_exits[\"Daily_Exits\"] = daily_exits.apply(get_daily_counts, axis=1, args=(1000000, ['Exits', 'Prev_exits']))\n",
    "    \n",
    "    daily = pd.merge(daily_entries, daily_exits, on=['C_A','Unit','SCP', 'Station', 'Date', 'Prev_date'])\n",
    "    daily['Total_throughput'] = daily['Daily_Entries'] + daily['Daily_Exits']\n",
    "    \n",
    "    return (daily\n",
    "             .groupby(['Station', 'Date'])\n",
    "             .sum()\n",
    "            #  .sort_values(by=['Total_throughput'], ascending=False)\n",
    "             .loc[:,['Daily_Entries', 'Daily_Exits', 'Total_throughput']]\n",
    "            #  .frame()\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calcHourThroughput(frame):\n",
    "    shift = frame.copy()\n",
    "\n",
    "    shift[['Datetime_Prev', 'Entries_Prev', 'Exits_Prev']] = (shift\n",
    "                .groupby(['C_A', 'Unit', 'SCP', 'Station'])['Datetime', 'Entries', 'Exits']\n",
    "                .transform(lambda grp: grp.shift(1)))\n",
    "    \n",
    "    shift['Entries'] = shift['Entries'] - shift['Entries_Prev']\n",
    "    shift['Exits'] = shift['Exits'] - shift['Exits_Prev']\n",
    "    shift = shift.dropna(how = 'any')\n",
    "    \n",
    "    shift['Throughput'] = shift['Entries'] + shift['Exits']\n",
    "    \n",
    "    return shift.loc[:,['Station','Datetime','Throughput']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make note to save and open pickles here:\n",
    "# dfutil.save_dataframe_as_pickle(df,'MTA_DATA_SPRING_2014_to_2016_FULL.pickle')\n",
    "# df_pickle = dfutil.open_dataframe_pickle('MTA_DATA_SPRING_2014_to_2016_FULL.pickle') # returns a df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# To find hour through put \n",
    "# test_df3.groupby(['Station', 'Datetime']).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>C_A</th>\n",
       "      <th>Unit</th>\n",
       "      <th>SCP</th>\n",
       "      <th>Station</th>\n",
       "      <th>Linename</th>\n",
       "      <th>Division</th>\n",
       "      <th>Date</th>\n",
       "      <th>TIME</th>\n",
       "      <th>Desc</th>\n",
       "      <th>Entries</th>\n",
       "      <th>Exits</th>\n",
       "      <th>Datetime</th>\n",
       "      <th>Day_of_week</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A002</td>\n",
       "      <td>R051</td>\n",
       "      <td>02-00-00</td>\n",
       "      <td>59 ST</td>\n",
       "      <td>NQR456</td>\n",
       "      <td>BMT</td>\n",
       "      <td>06/18/2016</td>\n",
       "      <td>00:00:00</td>\n",
       "      <td>REGULAR</td>\n",
       "      <td>5704958</td>\n",
       "      <td>1934814</td>\n",
       "      <td>2016-06-18 00:00:00</td>\n",
       "      <td>Saturday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A002</td>\n",
       "      <td>R051</td>\n",
       "      <td>02-00-00</td>\n",
       "      <td>59 ST</td>\n",
       "      <td>NQR456</td>\n",
       "      <td>BMT</td>\n",
       "      <td>06/18/2016</td>\n",
       "      <td>04:00:00</td>\n",
       "      <td>REGULAR</td>\n",
       "      <td>5704958</td>\n",
       "      <td>1934829</td>\n",
       "      <td>2016-06-18 04:00:00</td>\n",
       "      <td>Saturday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A002</td>\n",
       "      <td>R051</td>\n",
       "      <td>02-00-00</td>\n",
       "      <td>59 ST</td>\n",
       "      <td>NQR456</td>\n",
       "      <td>BMT</td>\n",
       "      <td>06/18/2016</td>\n",
       "      <td>08:00:00</td>\n",
       "      <td>REGULAR</td>\n",
       "      <td>5704958</td>\n",
       "      <td>1934886</td>\n",
       "      <td>2016-06-18 08:00:00</td>\n",
       "      <td>Saturday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A002</td>\n",
       "      <td>R051</td>\n",
       "      <td>02-00-00</td>\n",
       "      <td>59 ST</td>\n",
       "      <td>NQR456</td>\n",
       "      <td>BMT</td>\n",
       "      <td>06/18/2016</td>\n",
       "      <td>12:00:00</td>\n",
       "      <td>REGULAR</td>\n",
       "      <td>5704958</td>\n",
       "      <td>1934993</td>\n",
       "      <td>2016-06-18 12:00:00</td>\n",
       "      <td>Saturday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A002</td>\n",
       "      <td>R051</td>\n",
       "      <td>02-00-00</td>\n",
       "      <td>59 ST</td>\n",
       "      <td>NQR456</td>\n",
       "      <td>BMT</td>\n",
       "      <td>06/18/2016</td>\n",
       "      <td>16:00:00</td>\n",
       "      <td>REGULAR</td>\n",
       "      <td>5704958</td>\n",
       "      <td>1935069</td>\n",
       "      <td>2016-06-18 16:00:00</td>\n",
       "      <td>Saturday</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    C_A  Unit       SCP Station Linename Division        Date      TIME  \\\n",
       "0  A002  R051  02-00-00   59 ST   NQR456      BMT  06/18/2016  00:00:00   \n",
       "1  A002  R051  02-00-00   59 ST   NQR456      BMT  06/18/2016  04:00:00   \n",
       "2  A002  R051  02-00-00   59 ST   NQR456      BMT  06/18/2016  08:00:00   \n",
       "3  A002  R051  02-00-00   59 ST   NQR456      BMT  06/18/2016  12:00:00   \n",
       "4  A002  R051  02-00-00   59 ST   NQR456      BMT  06/18/2016  16:00:00   \n",
       "\n",
       "      Desc  Entries    Exits            Datetime Day_of_week  \n",
       "0  REGULAR  5704958  1934814 2016-06-18 00:00:00    Saturday  \n",
       "1  REGULAR  5704958  1934829 2016-06-18 04:00:00    Saturday  \n",
       "2  REGULAR  5704958  1934886 2016-06-18 08:00:00    Saturday  \n",
       "3  REGULAR  5704958  1934993 2016-06-18 12:00:00    Saturday  \n",
       "4  REGULAR  5704958  1935069 2016-06-18 16:00:00    Saturday  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Daily_Entries</th>\n",
       "      <th>Daily_Exits</th>\n",
       "      <th>Total_throughput</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Station</th>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">1 AVE</th>\n",
       "      <th>02/28/2015</th>\n",
       "      <td>2149122.0</td>\n",
       "      <td>341314.0</td>\n",
       "      <td>2490436.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>02/28/2016</th>\n",
       "      <td>2153712.0</td>\n",
       "      <td>342328.0</td>\n",
       "      <td>2496040.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>02/29/2016</th>\n",
       "      <td>14018.0</td>\n",
       "      <td>15107.0</td>\n",
       "      <td>29125.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>03/01/2015</th>\n",
       "      <td>2151936.0</td>\n",
       "      <td>342214.0</td>\n",
       "      <td>2494150.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>03/01/2016</th>\n",
       "      <td>2156668.0</td>\n",
       "      <td>343143.0</td>\n",
       "      <td>2499811.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Daily_Entries  Daily_Exits  Total_throughput\n",
       "Station Date                                                    \n",
       "1 AVE   02/28/2015      2149122.0     341314.0         2490436.0\n",
       "        02/28/2016      2153712.0     342328.0         2496040.0\n",
       "        02/29/2016        14018.0      15107.0           29125.0\n",
       "        03/01/2015      2151936.0     342214.0         2494150.0\n",
       "        03/01/2016      2156668.0     343143.0         2499811.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Station</th>\n",
       "      <th>Datetime</th>\n",
       "      <th>Throughput</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>59 ST</td>\n",
       "      <td>2016-06-18 04:00:00</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>59 ST</td>\n",
       "      <td>2016-06-18 08:00:00</td>\n",
       "      <td>57.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>59 ST</td>\n",
       "      <td>2016-06-18 12:00:00</td>\n",
       "      <td>107.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>59 ST</td>\n",
       "      <td>2016-06-18 16:00:00</td>\n",
       "      <td>76.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>59 ST</td>\n",
       "      <td>2016-06-18 20:00:00</td>\n",
       "      <td>63.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Station            Datetime  Throughput\n",
       "1   59 ST 2016-06-18 04:00:00        15.0\n",
       "2   59 ST 2016-06-18 08:00:00        57.0\n",
       "3   59 ST 2016-06-18 12:00:00       107.0\n",
       "4   59 ST 2016-06-18 16:00:00        76.0\n",
       "5   59 ST 2016-06-18 20:00:00        63.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "htp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "htp.groupby(['Station', 'Datetime']).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "\n",
    "Day of the week - 7 days \n",
    "- index'd by rank\n",
    "- names of stations \n",
    "- each column is a time of the day\n",
    "\n",
    "map of morning, afternoon, evening\n",
    "     for each day of week\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration \n",
    "::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The column names contain unneeded whitespace.\n",
    "df1.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The dataframe also lacks a timeseries.\n",
    "df1.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Strip whitespace from column names\n",
    "df1.columns = [s.strip() for s in df1.columns.values]\n",
    "\n",
    "# Create datetime column with datetime datatype\n",
    "df1['Datetime'] = pd.to_datetime(df1.DATE + ' ' + df1.TIME,\n",
    "                                 format = '%m/%d/%Y %H:%M:%S')\n",
    "\n",
    "df1.iloc[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Chain methods together to further clean data:\n",
    "    # drop old date and time columns\n",
    "    # rename columns\n",
    "    \n",
    "dict_col_rename = {'C/A' : 'C_A', 'UNIT' : 'Unit', 'STATION' : 'Station', 'LINENAME' : 'Linename',\n",
    "                  'DIVISION' : 'Division', 'DESC' : 'Desc', 'ENTRIES' : 'Entries', 'EXITS' : 'Exits',\n",
    "                  'DATE' : 'Date'}    \n",
    "\n",
    "df2 = (df1\n",
    "       #.drop('DATE', axis = 1)\n",
    "       .drop('TIME', axis = 1)\n",
    "       .rename(columns = dict_col_rename)\n",
    "      )\n",
    "\n",
    "df2.iloc[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing Erroneous Observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Check uniqueness of rows/indexes by getting counts.\n",
    "(df2\n",
    " .groupby(['C_A', 'Unit', 'SCP', 'Station', 'Datetime'])\n",
    " .count() #.Entries.count()\n",
    " .reset_index()\n",
    " .sort_values('Entries', ascending = False)\n",
    " .iloc[:5]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# On 3/25, we don't seem to have two entries for same time, but let's take a look anyway.\n",
    "\n",
    "mask = ((df2[\"C_A\"] == \"A002\") & \n",
    "(df2[\"Unit\"] == \"R051\") & \n",
    "(df2[\"SCP\"] == \"02-00-00\") & \n",
    "(df2[\"Station\"] == \"59 ST\") &\n",
    "(df2[\"Datetime\"].dt.date == datetime.datetime(2017, 3, 25).date()))\n",
    "df2[mask].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df2.Desc.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Questions for the future, if there is time:\n",
    "    # Are there other values of DESC?\n",
    "    # Are there other fields to check for odd values?\n",
    "\n",
    "# Drop duplicates.\n",
    "df_no_dupe = df2.drop_duplicates(subset=['C_A', 'Unit', 'SCP', 'Station', 'Datetime'])\n",
    "\n",
    "# Check uniqueness again after data cleaning to confirm cleanness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_no_dupe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating the Entries and Exits per Day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_daily_entries = (df_no_dupe\n",
    "            .groupby(['C_A', 'Unit', 'SCP', 'Station', 'Date'])\n",
    "            .Entries\n",
    "            .first()\n",
    "            .reset_index()\n",
    "           )\n",
    "\n",
    "df_daily_exits = (df_no_dupe\n",
    "            .groupby(['C_A', 'Unit', 'SCP', 'Station', 'Date'])\n",
    "            .Exits\n",
    "            .first()\n",
    "            .reset_index()\n",
    "           )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Calculate the differences by day\n",
    "df_daily_entries[[\"Prev_date\", \"Prev_entries\"]] = (df_daily_entries\n",
    "                                                       .groupby([\"C_A\", \"Unit\", \"SCP\", \"Station\"])[\"Date\", \"Entries\"]\n",
    "                                                       .transform(lambda grp: grp.shift(1)))\n",
    "\n",
    "df_daily_exits[[\"Prev_date\", \"Prev_exits\"]]   = (df_daily_exits\n",
    "                                                       .groupby([\"C_A\", \"Unit\", \"SCP\", \"Station\"])[\"Date\", \"Exits\"]\n",
    "                                                       .transform(lambda grp: grp.shift(1)))\n",
    "\n",
    "# Drop all the null values generated above\n",
    "df_daily_entries.dropna(subset=[\"Prev_date\"], axis=0, inplace=True)\n",
    "df_daily_exits.dropna(subset=[\"Prev_date\"], axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_daily_entries.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Check for any counters that have been reversed\n",
    "df_daily_entries[df_daily_entries[\"Entries\"] < df_daily_entries[\"Prev_entries\"]].head()\n",
    "\n",
    "# WTC: Is this something that can be solved by sorting before applying the transform above?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##### HAVE NOT EDITED YET\n",
    "\n",
    "# Pick a value from one of the counters reversed above & check for it \n",
    "# What's the deal with counter being in reverse\n",
    "# mask = ((turnstiles_df[\"C/A\"] == \"A011\") & \n",
    "# (turnstiles_df[\"UNIT\"] == \"R080\") & \n",
    "# (turnstiles_df[\"SCP\"] == \"01-00-00\") & \n",
    "# (turnstiles_df[\"STATION\"] == \"57 ST-7 AV\") &\n",
    "# (turnstiles_df[\"DATE_TIME\"].dt.date == datetime.datetime(2016, 8, 27).date()))\n",
    "# turnstiles_df[mask].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Let's see how many stations have this problem\n",
    "\n",
    "(df_daily_entries[df_daily_entries[\"Entries\"] < df_daily_entries[\"Prev_entries\"]]\n",
    "    .groupby([\"C_A\", \"Unit\", \"SCP\", \"Station\"])\n",
    "    .size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# def get_daily_counts(row, max_counter, cols):\n",
    "#     counter = row[cols[0]] - row[cols[1]]\n",
    "#     if counter < 0:\n",
    "#         # May be counter is reversed?\n",
    "#         counter = -counter\n",
    "#     if counter > max_counter:\n",
    "#         print(row[cols[0]], row[cols[1]])\n",
    "#         counter = min(row[cols[0]], row[cols[1]])\n",
    "#     if counter > max_counter:\n",
    "#         # Check it again to make sure we are not giving a counter that's too big\n",
    "#         return 0\n",
    "#     return counter\n",
    "\n",
    "# If counter is > 1Million, then the counter might have been reset.  \n",
    "# Just set it to zero as different counters have different cycle limits\n",
    "df_daily_entries[\"Daily_Entries\"] = df_daily_entries.apply(get_daily_counts, axis=1, args=(1000000, ['Entries', 'Prev_entries']))\n",
    "df_daily_exits[\"Daily_Exits\"] = df_daily_exits.apply(get_daily_counts, axis=1, args=(1000000, ['Exits', 'Prev_exits']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_daily_entries.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_daily_exits.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_daily = pd.merge(df_daily_entries, df_daily_exits, on=['C_A','Unit','SCP', 'Station', 'Date', 'Prev_date'])\n",
    "df_daily['Total_throughput'] = df_daily['Daily_Entries'] + df_daily['Daily_Exits']\n",
    "df_daily.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(df_daily\n",
    " .groupby(['Station', 'Date'])\n",
    " .sum()\n",
    "#  .sort_values(by=['Total_throughput'], ascending=False)\n",
    " .loc[:,['Daily_Entries', 'Daily_Exits', 'Total_throughput']]\n",
    "#  .frame()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating the Entries and Exits per Hour\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Use concat (as a join) to fix (\"de-cumulate\") the columns Entries and Exits\n",
    "\n",
    "df_shift = (df_no_dupe\n",
    "            .copy()\n",
    "            .drop('Linename', axis = 1)\n",
    "            .drop('Division', axis = 1)\n",
    "           )\n",
    "\n",
    "df_shift[['Datetime_Prev', 'Entries_Prev', 'Exits_Prev']] = (df_shift\n",
    "            .groupby(['C_A', 'Unit', 'SCP', 'Station'])['Datetime', 'Entries', 'Exits']\n",
    "            .transform(lambda grp: grp.shift(1)))\n",
    "\n",
    "df_shift.head()\n",
    "\n",
    "\n",
    "# Legacy\n",
    "\n",
    "# df_shift.columns\n",
    "# df_shift['Datetime_Prev', 'Entries_Prev', 'Exits_Prev'] = (\n",
    "#     df_no_dupe#[['C_A', 'Unit', 'SCP', 'Station', 'Datetime', 'Entries', 'Exits']]\n",
    "#             .groupby(['C_A', 'Unit', 'SCP', 'Station'])['Datetime', 'Entries', 'Exits']\n",
    "#             #.groupby([\"C/A\", \"UNIT\", \"SCP\", \"STATION\"])[\"DATE\", \"ENTRIES\"]\n",
    "#             .transform(lambda grp: grp.shift(1))\n",
    "#             #.shift(periods = 1)\n",
    "#             #.rename(columns = {'Entries' : 'Entries_Shift', 'Exits' : 'Exits_Shift', \n",
    "#             #                   'Datetime' : 'Prev_datetime'})\n",
    "#            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_shift['Entries'] = df_shift['Entries'] - df_shift['Entries_Prev']\n",
    "df_shift['Exits'] = df_shift['Exits'] - df_shift['Exits_Prev']\n",
    "df_shift = df_shift.dropna(how = 'any')\n",
    "\n",
    "df_shift.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Challenge 3 - Total Daily Entries\n",
    "#df3or4['Datetime'].dt.date == datetime.datetime(YYYY, MM, DD).date()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def get_daily_counts(row, max_counter):\n",
    "#     counter = abs(row[\"ENTRIES\"] - row[\"PREV_ENTRIES\"])\n",
    "    \n",
    "#     if counter > max_counter:\n",
    "#         print(row[\"ENTRIES\"], row[\"PREV_ENTRIES\"])\n",
    "#         return 0\n",
    "#     return counter\n",
    "\n",
    "# # If counter is > 1Million, then the counter might have been reset.  \n",
    "# # Just set it to zero as different counters have different cycle limits\n",
    "# _ = turnstiles_daily.apply(get_daily_counts, axis=1, max_counter=1000000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
